use anyhow::{anyhow, Result};
use aws_sdk_cloudformation::Client as CloudFormationClient;
use aws_sdk_lambda::{error::ProvideErrorMetadata, Client as LambdaClient};
use base64::{engine::general_purpose, Engine};
use chrono::Local;
use futures::stream::{self, StreamExt, TryStreamExt};
use indicatif::{ProgressBar, ProgressStyle};
use std::{
    collections::HashMap,
    fs::{self, File},
    io::Write,
    path::PathBuf,
    time::Duration,
    sync::atomic::{AtomicBool, Ordering},
};
use opentelemetry::trace::SpanKind;
use tracing::Span;
use opentelemetry_http::HeaderInjector;
use reqwest::header::HeaderMap;
use serde_json::Value;
use tracing_opentelemetry::OpenTelemetrySpanExt;
use aws_sdk_lambda::primitives::Blob;

use crate::types::*;

static INTERRUPTED: AtomicBool = AtomicBool::new(false);

pub fn clear_interrupt() {
    INTERRUPTED.store(false, Ordering::SeqCst);
}

pub fn is_interrupted() -> bool {
    INTERRUPTED.load(Ordering::SeqCst)
}

#[derive(Clone)]
struct FunctionBenchmarkConfig {
    function_name: String,
    memory_size: Option<i32>,
    concurrent: u32,
    rounds: u32,
    payload: Option<String>,
    #[allow(dead_code)]
    output_dir: String,
    environment: Vec<(String, String)>,
    proxy_function: Option<String>,
}

impl FunctionBenchmarkConfig {
    #[allow(clippy::too_many_arguments)]
    fn new(
        function_name: impl Into<String>,
        memory_size: Option<i32>,
        concurrent: u32,
        rounds: u32,
        payload: Option<String>,
        output_dir: impl Into<String>,
        environment: Vec<(String, String)>,
        proxy_function: Option<String>,
    ) -> Self {
        Self {
            function_name: function_name.into(),
            memory_size,
            concurrent,
            rounds,
            payload,
            output_dir: output_dir.into(),
            environment,
            proxy_function,
        }
    }
}

#[tracing::instrument(
    name = "invoke_function",
    fields(
        function.name = %function_name,
        function.memory = memory_size.unwrap_or(128),
        otel.kind = ?SpanKind::Client
    ),
    skip(client, _environment)
)]
pub async fn invoke_function(
    client: &LambdaClient,
    function_name: &str,
    memory_size: Option<i32>,
    payload: Option<&str>,
    _environment: &[(String, String)],
    skip_logs: bool,
    proxy_function: Option<&str>,
) -> Result<InvocationMetrics> {
    let span = Span::current();
    
    let mut req = client.invoke();

    // Only request logs if not skipping
    if !skip_logs {
        req = req.log_type(aws_sdk_lambda::types::LogType::Tail);
    }

    // Inject trace context into payload
    let mut final_payload = if let Some(p) = payload {
        // If payload exists, parse it
        serde_json::from_str(p)?
    } else {
        // If no payload, create empty object
        serde_json::Value::Object(serde_json::Map::new())
    };

    // Create a map for trace headers
    let mut trace_headers = HeaderMap::new();
    let mut injector = HeaderInjector(&mut trace_headers);
    let current_span: Span = Span::current();
    let cx = current_span.context();
    opentelemetry::global::get_text_map_propagator(|propagator| {
        propagator.inject_context(&cx, &mut injector);
    });
        
    // Create headers object with trace context
    let mut otel_context = serde_json::Map::new();
    let mut has_trace_context = false;
    
    if let Some(traceparent) = trace_headers.get("traceparent") {
        has_trace_context = true;
        otel_context.insert(
            "traceparent".to_string(),
            Value::String(traceparent.to_str().unwrap().to_string())
        );
    }
    if let Some(tracestate) = trace_headers.get("tracestate") {
        otel_context.insert(
            "tracestate".to_string(),
            Value::String(tracestate.to_str().unwrap().to_string())
        );
    }

    // Add headers object to payload only if we have trace context
    if has_trace_context {
        if let Value::Object(ref mut map) = final_payload {
            map.insert(
                "headers".to_string(),
                Value::Object(otel_context)
            );
        }
    }

    // Start client-side timing only if we're measuring client metrics
    let start = if skip_logs { Some(std::time::Instant::now()) } else { None };

    let result = if skip_logs && proxy_function.is_some() {
        // When doing client measurements and proxy is available, use it
        let proxy = proxy_function.unwrap();
        let proxy_request = ProxyRequest {
            target: function_name.to_string(),
            payload: final_payload,
        };

        req.function_name(proxy)
            .payload(Blob::new(serde_json::to_vec(&proxy_request)?))
            .send()
            .await
    } else {
        // Direct invocation for:
        // 1. Server metrics (skip_logs = false)
        // 2. Client measurements without proxy
        req.function_name(function_name)
            .payload(Blob::new(final_payload.to_string()))
            .send()
            .await
    };

    match result {
        Ok(output) => {
            // Calculate client-side duration if we're measuring it
            let client_duration = start.map(|s| s.elapsed().as_secs_f64() * 1000.0).unwrap_or(0.0);
            
            // Record duration in span
            span.set_attribute("client.duration_ms", client_duration);

            if skip_logs {
                // When skipping logs, just return client duration with current timestamp
                Ok(InvocationMetrics {
                    timestamp: chrono::Utc::now().format("%Y-%m-%dT%H:%M:%S%.3fZ").to_string(),
                    client_duration: if proxy_function.is_some() {
                        let proxy_response: ProxyResponse = serde_json::from_slice(output.payload()
                            .ok_or_else(|| anyhow!("No response from proxy function"))?.as_ref())?;
                        proxy_response.invocation_time_ms
                    } else {
                        client_duration
                    },
                    init_duration: None,
                    duration: 0.0,
                    net_duration: 0.0,
                    billed_duration: 0,
                    memory_size: memory_size.unwrap_or(128) as i64,
                    max_memory_used: 0,
                })
            } else {
                // Extract and decode base64 logs
                let logs = output
                    .log_result()
                    .ok_or_else(|| anyhow!("No logs returned"))?;
                let decoded_logs = String::from_utf8(
                    general_purpose::STANDARD
                        .decode(logs)
                        .expect("Failed to decode base64 payload"),
                )
                .expect("Failed to decode logs");
                // Check for function error and include decoded logs in error message
                if let Some(func_error) = output.function_error() {
                    span.set_attribute("error", true);
                    span.set_attribute("error.type", func_error.to_string());
                    return Err(anyhow!(
                        "Function invocation failed: {}.\nLogs:\n{}",
                        func_error,
                        decoded_logs
                    ));
                }

                // Get server-side metrics
                let mut metrics = extract_metrics(&decoded_logs)?;
                
                // Add metrics to span
                span.set_attribute("function.duration_ms", metrics.duration);
                span.set_attribute("function.billed_duration_ms", metrics.billed_duration);
                span.set_attribute("function.net_duration_ms", metrics.net_duration);
                if let Some(init) = metrics.init_duration {
                    span.set_attribute("function.init_duration_ms", init);
                }
                
                // Add client-side duration if we measured it
                metrics.client_duration = client_duration;
                Ok(metrics)
            }
        }
        Err(err) => {
            span.set_attribute("error", true);
            let error_details = match err {
                aws_sdk_lambda::error::SdkError::ServiceError(context) => {
                    let msg = format!(
                        "Service error: {} ({})",
                        context.err().message().unwrap_or_default(),
                        context.err().code().unwrap_or_default()
                    );
                    span.set_attribute("error.type", "service_error");
                    span.set_attribute("error.message", msg.clone());
                    msg
                }
                other_err => {
                    let msg = format!("SDK error: {}", other_err);
                    span.set_attribute("error.type", "sdk_error");
                    span.set_attribute("error.message", msg.clone());
                    msg
                }
            };
            
            Err(anyhow!(
                "Failed to invoke function: {}",
                error_details
            ))
        }
    }
}

pub fn extract_metrics(logs: &str) -> Result<InvocationMetrics> {
    // Find the last platform.report line
    let report = logs
        .lines()
        .filter_map(|line| {
            // Try to parse each line as JSON
            serde_json::from_str::<PlatformReport>(line).ok()
        })
        .filter(|report| report.report_type == "platform.report")
        .last()
        .ok_or_else(|| anyhow!("No platform.report found in logs"))?;

    // Find extension overhead if present
    let extension_overhead = report
        .record
        .spans
        .iter()
        .find(|span| span.name == "extensionOverhead")
        .map_or(0.0, |span| span.duration_ms);

    // Calculate net duration by subtracting extension overhead
    let duration = report.record.metrics.duration_ms;
    let net_duration = duration - extension_overhead;

    Ok(InvocationMetrics {
        timestamp: report.time.clone(),
        client_duration: 0.0,  // This will be set by invoke_function
        init_duration: report.record.metrics.init_duration_ms,
        duration,
        net_duration,
        billed_duration: report.record.metrics.billed_duration_ms,
        memory_size: report.record.metrics.memory_size_mb,
        max_memory_used: report.record.metrics.max_memory_used_mb,
    })
}

pub async fn save_report(report: BenchmarkReport, output_dir: &str) -> Result<()> {
    let memory_dir = report
        .config
        .memory_size
        .map_or("default".to_string(), |m| format!("{}mb", m));
    let output_path = PathBuf::from(output_dir).join(&memory_dir);
    fs::create_dir_all(&output_path)?;

    let function_name = report.config.function_name.clone();
    let filename = format!("{}.json", function_name);
    let output_path = output_path.join(filename);

    let json = serde_json::to_string_pretty(&report)?;
    let mut file = File::create(&output_path)?;
    file.write_all(json.as_bytes())?;
    println!("\nReport saved to: {}", output_path.display());
    Ok(())
}

#[derive(Default)]
pub struct BenchmarkResults {
    pub cold_starts: Vec<InvocationMetrics>,
    pub warm_starts: Vec<InvocationMetrics>,
    pub client_measurements: Vec<InvocationMetrics>,
}

async fn run_benchmark_pass(
    client: &LambdaClient,
    config: &FunctionBenchmarkConfig,
    client_measurement: bool,
) -> Result<BenchmarkResults> {
    use tokio::signal;

    let mut results = BenchmarkResults {
        cold_starts: Vec::new(),
        warm_starts: Vec::new(),
        client_measurements: Vec::new(),
    };

    // Cold starts - run concurrently
    let mut handles = Vec::new();
    for _ in 0..config.concurrent {
        let client = client.clone();
        let function_name = config.function_name.clone();
        let payload = config.payload.clone();
        let environment = config.environment.clone();
        let memory_size = config.memory_size;
        let proxy_function = config.proxy_function.clone();
        
        handles.push(tokio::spawn(async move {
            invoke_function(
                &client,
                &function_name,
                memory_size,
                payload.as_deref(),
                &environment,
                client_measurement,
                proxy_function.as_deref(),
            )
            .await
        }));
    }

    // Wait for cold starts with Ctrl-C handling
    let cold_start_future = async {
        for handle in handles {
            let metrics = handle.await??;
            results.cold_starts.push(metrics);
        }
        Ok::<(), anyhow::Error>(())
    };

    // Handle Ctrl-C during cold starts
    if tokio::select! {
        result = cold_start_future => result.is_err(),
        _ = signal::ctrl_c() => {
            println!("\n\nReceived Ctrl-C, interrupting...");
            INTERRUPTED.store(true, Ordering::SeqCst);
            true
        }
    } {
        return Ok(results);
    }

    // Setup progress bar for warm starts
    let progress = if config.rounds > 1 {
        let pb = ProgressBar::new(config.rounds as u64);
        pb.set_style(
            ProgressStyle::default_bar()
                .template("{spinner:.green} [{elapsed_precise}] [{bar:40.cyan/blue}] {pos}/{len} rounds")
                .unwrap()
                .progress_chars("#>-"),
        );
        Some(pb)
    } else {
        None
    };

    // Warm starts with Ctrl-C handling
    for _round in 1..=config.rounds {
        let mut handles = Vec::new();
        for _ in 0..config.concurrent {
            let client = client.clone();
            let function_name = config.function_name.clone();
            let payload = config.payload.clone();
            let environment = config.environment.clone();
            let memory_size = config.memory_size;
            let proxy_function = config.proxy_function.clone();
            
            handles.push(tokio::spawn(async move {
                invoke_function(
                    &client,
                    &function_name,
                    memory_size,
                    payload.as_deref(),
                    &environment,
                    client_measurement,
                    proxy_function.as_deref(),
                )
                .await
            }));
        }

        // Handle Ctrl-C for each round of warm starts
        let warm_start_future = async {
            for handle in handles {
                let metrics = handle.await??;
                results.warm_starts.push(metrics);
            }
            Ok::<(), anyhow::Error>(())
        };

        if tokio::select! {
            result = warm_start_future => result.is_err(),
            _ = signal::ctrl_c() => {
                println!("\n\nReceived Ctrl-C, interrupting...");
                if let Some(pb) = &progress {
                    pb.finish_and_clear();
                }
                INTERRUPTED.store(true, Ordering::SeqCst);
                true
            }
        } {
            return Ok(results);
        }

        if let Some(pb) = &progress {
            pb.inc(1);
        }
    }

    // Finish progress bar
    if let Some(pb) = progress {
        pb.finish_and_clear();
    }

    Ok(results)
}

/// Get the original function configuration
async fn get_function_config(client: &LambdaClient, function_name: &str) -> Result<OriginalConfig> {
    let function = client
        .get_function()
        .function_name(function_name)
        .send()
        .await
        .map_err(|err| {
            if err.to_string().contains("ResourceNotFoundException") {
                anyhow!("Function '{}' not found", function_name)
            } else {
                anyhow!(
                    "Something went wrong: {}. Error getting function configuration. Please check your AWS configuration",
                    err
                )
            }
        })?;

    let config = function.configuration().ok_or_else(|| {
        anyhow!("Failed to get function configuration for '{}'", function_name)
    })?;

    Ok(OriginalConfig {
        memory_size: config.memory_size().unwrap_or(128) as i32,
        environment: config
            .environment()
            .and_then(|e| e.variables())
            .map(|vars| vars.iter().map(|(k, v)| (k.clone(), v.clone())).collect())
            .unwrap_or_default(),
    })
}

/// Update function configuration
async fn update_function_config(
    client: &LambdaClient,
    function_name: &str,
    memory_size: Option<i32>,
    environment: &[(String, String)],
) -> Result<()> {
    // Get current configuration
    let function = client
        .get_function()
        .function_name(function_name)
        .send()
        .await
        .map_err(|err| {
            if err.to_string().contains("ResourceNotFoundException") {
                anyhow!("Function '{}' not found", function_name)
            } else {
                anyhow!(
                    "Something went wrong: {}. Error getting function configuration. Please check your AWS configuration",
                    err
                )
            }
        })?;

    // Get current configuration
    let current_config = function.configuration().ok_or_else(|| {
        anyhow!("Failed to get function configuration for '{}'", function_name)
    })?;

    // Build update configuration
    let mut update = client.update_function_configuration().function_name(function_name);
    
    // Add memory if specified
    if let Some(memory) = memory_size {
        update = update.memory_size(memory);
    }

    // Prepare environment variables
    let mut env_vars = HashMap::new();
    
    // First, copy existing environment variables
    if let Some(current_env) = current_config.environment().and_then(|e| e.variables()) {
        env_vars.extend(current_env.iter().map(|(k, v)| (k.clone(), v.clone())));
    }
    
    // Then add/update new variables
    for (key, value) in environment {
        env_vars.insert(key.clone(), value.clone());
    }

    // Only set environment if we have variables
    if !env_vars.is_empty() {
        update = update.environment(
            aws_sdk_lambda::types::Environment::builder()
                .set_variables(Some(env_vars))
                .build(),
        );
    }

    // Send update
    match update.send().await {
        Ok(_) => {
            // Wait for function update to complete
            tokio::time::sleep(Duration::from_secs(5)).await;
            Ok(())
        }
        Err(err) => {
            let error_details = match err {
                aws_sdk_lambda::error::SdkError::ServiceError(context) => format!(
                    "Service error: {} ({})",
                    context.err().message().unwrap_or_default(),
                    context.err().code().unwrap_or_default()
                ),
                other_err => format!("SDK error: {}", other_err),
            };
            
            Err(anyhow!(
                "Failed to update function configuration: {}",
                error_details
            ))
        }
    }
}

/// Restore original function configuration
async fn restore_function_config(
    client: &LambdaClient,
    function_name: &str,
    original: &OriginalConfig,
) -> Result<()> {
    println!("\nRestoring function configuration...");
    update_function_config(
        client,
        function_name,
        Some(original.memory_size),
        &original.environment.iter().map(|(k, v)| (k.clone(), v.clone())).collect::<Vec<_>>(),
    ).await?;
    println!("‚úì Function configuration restored");
    Ok(())
}

#[allow(clippy::too_many_arguments)]
pub async fn run_function_benchmark(
    client: &LambdaClient,
    function_name: &str,
    memory_size: Option<i32>,
    concurrent: u32,
    rounds: u32,
    payload: Option<&str>,
    output_dir: &str,
    environment: &[(&str, &str)],
    client_metrics: bool,
    proxy_function: Option<&str>,
) -> Result<()> {

    println!("\nStarting benchmark for: {}", function_name);
    
    // Get function configuration to extract runtime and architecture
    let function = client
        .get_function()
        .function_name(function_name)
        .send()
        .await
        .map_err(|err| {
            if err.to_string().contains("ResourceNotFoundException") {
                anyhow!("Function '{}' not found", function_name)
            } else {
                anyhow!(
                    "Something went wrong: {}. Error getting function configuration. Please check your AWS configuration",
                    err
                )
            }
        })?;

    let config = function.configuration().ok_or_else(|| {
        anyhow!("Failed to get function configuration for '{}'", function_name)
    })?;

    let runtime = config.runtime().map(|r| r.as_str().to_string());
    let architecture = if config.architectures().is_empty() {
        Some("x86_64".to_string())
    } else {
        config.architectures()
            .first()
            .map(|arch| arch.as_str().to_string())
    };

    println!("Configuration:");
    println!("  Memory: {} MB", memory_size.unwrap_or(128));
    println!("  Runtime: {}", runtime.as_deref().unwrap_or("unknown"));
    println!("  Architecture: {}", architecture.as_deref().unwrap_or("unknown"));
    println!("  Concurrency: {}", concurrent);
    println!("  Rounds: {}", rounds);
    if let Some(proxy) = proxy_function {
        println!("  Using Proxy Function: {}", proxy);
    }
    if !environment.is_empty() {
        println!("  Environment:");
        for (key, value) in environment {
            println!("    {}={}", key, value);
        }
    }

    // Print telemetry configuration
    println!("\nTelemetry:");
    if let (Ok(endpoint), Ok(service)) = (
        std::env::var("OTEL_EXPORTER_OTLP_ENDPOINT"),
        std::env::var("OTEL_SERVICE_NAME")
    ) {
        println!("  Service: {}", service);
        println!("  Endpoint: {}", endpoint);
        println!("  Protocol: {}", std::env::var("OTEL_EXPORTER_OTLP_PROTOCOL").unwrap_or_else(|_| "http/protobuf (default)".to_string()));
        
        // Region is required for AWS endpoints
        if endpoint.contains(".amazonaws.com") {
            let region = std::env::var("AWS_REGION").unwrap_or_else(|_| "us-east-1".to_string());
            println!("  Region: {}{}", region, if region == "us-east-1" { " *" } else { "" });
        }
    } else {
        println!("  OpenTelemetry is not configured (OTEL_EXPORTER_OTLP_ENDPOINT and OTEL_SERVICE_NAME are required)");
    }

    // Convert environment variables
    let env: Vec<(String, String)> = environment
        .iter()
        .map(|(k, v)| (k.to_string(), v.to_string()))
        .collect();

    // Save original configuration if we're going to modify it
    let original_config = if memory_size.is_some() || !environment.is_empty() {
        Some(get_function_config(client, function_name).await?)
    } else {
        None
    };

    // Update function configuration if needed
    if memory_size.is_some() || !environment.is_empty() {
        println!("\nUpdating function configuration...");
        update_function_config(client, function_name, memory_size, &env).await?;
        println!("‚úì Function configuration updated");
    }

    // Create a future for the benchmark execution
    let config = FunctionBenchmarkConfig::new(
        function_name.to_string(),
        memory_size,
        concurrent,
        rounds,
        payload.map(|s| s.to_string()),
        output_dir.to_string(),
        env,
        proxy_function.map(|s| s.to_string()),
    );

    let result = async {
        // First pass - get server metrics and cold start
        println!("\nCollecting server metrics...");
        let mut results = run_benchmark_pass(client, &config, false).await?;
        println!("‚úì Server metrics collected");

        // If client metrics requested, do a second pass for warm starts only
        if client_metrics {
            println!("\nCollecting client metrics...");
            // Run without logs to get accurate client metrics
            let client_results = run_benchmark_pass(client, &config, true).await?;
            results.client_measurements = client_results.warm_starts;
            println!("‚úì Client metrics collected");
        }

        // Print results
        print_benchmark_results(&results);

        // Save results
        save_report(BenchmarkReport {
            config: BenchmarkConfig {
                function_name: function_name.to_string(),
                memory_size,
                concurrent_invocations: concurrent,
                rounds,
                timestamp: Local::now().format("%Y-%m-%dT%H:%M:%S").to_string(),
                runtime,
                architecture,
                environment: environment.iter()
                    .map(|(k, v)| EnvVar { 
                        key: k.to_string(), 
                        value: v.to_string() 
                    })
                    .collect(),
            },
            cold_starts: results.cold_starts.iter()
                .filter_map(|m| m.to_cold_start())
                .collect(),
            warm_starts: results.warm_starts.iter()
                .map(|m| m.to_warm_start())
                .collect(),
            client_measurements: results.client_measurements.iter()
                .map(|m| m.to_client_metrics())
                .collect(),
        }, output_dir).await?;

        Ok(())
    }.await;

    // Restore original configuration if we modified it
    if let Some(original) = original_config {
        // Always try to restore, even if the benchmark failed or was interrupted
        if let Err(e) = restore_function_config(client, function_name, &original).await {
            eprintln!("Warning: Failed to restore function configuration: {}", e);
        }
    }

    // Now return the benchmark result
    result
}

// Move the results printing to a separate function
fn print_benchmark_results(results: &BenchmarkResults) {
    if !results.cold_starts.is_empty() && results.cold_starts.iter().any(|m| m.init_duration.is_some()) {
        println!(
            "\nü•∂ Cold Start Metrics ({} invocations) | Memory Size: {} MB",
            results.cold_starts.len(),
            results.cold_starts[0].memory_size
        );
        println!(
            "\n{:<15} {:>12} {:>12} {:>12} {:>12} {:>12}",
            "Metric", "Min", "Max", "Mean", "P50", "P95"
        );
        println!("{:-<87}", "");

        let init_durations: Vec<f64> = results
            .cold_starts
            .iter()
            .filter_map(|m| m.init_duration)
            .collect();
        if !init_durations.is_empty() {
            print_stats(&calculate_stats(&init_durations), "Init Duration");
        }

        let durations: Vec<f64> = results.cold_starts.iter().map(|m| m.duration).collect();
        print_stats(&calculate_stats(&durations), "Server Duration");

        let net_durations: Vec<f64> = results.cold_starts.iter().map(|m| m.net_duration).collect();
        print_stats(&calculate_stats(&net_durations), "Net Duration");

        let billed_durations: Vec<f64> = results
            .cold_starts
            .iter()
            .map(|m| m.billed_duration as f64)
            .collect();
        print_stats(&calculate_stats(&billed_durations), "Billed Duration");

        let memory_used: Vec<f64> = results
            .cold_starts
            .iter()
            .map(|m| m.max_memory_used as f64)
            .collect();
        print_memory_stats(&calculate_stats(&memory_used), "Memory Used");
    }

    if !results.warm_starts.is_empty() {
        println!(
            "\nüî• Warm Start Metrics ({} invocations) | Memory Size: {} MB",
            results.warm_starts.len(),
            results.warm_starts[0].memory_size
        );
        println!(
            "\n{:<15} {:>12} {:>12} {:>12} {:>12} {:>12}",
            "Metric", "Min", "Max", "Mean", "P50", "P95"
        );
        println!("{:-<87}", "");

        let durations: Vec<f64> = results.warm_starts.iter().map(|m| m.duration).collect();
        print_stats(&calculate_stats(&durations), "Server Duration");

        let net_durations: Vec<f64> = results.warm_starts.iter().map(|m| m.net_duration).collect();
        print_stats(&calculate_stats(&net_durations), "Net Duration");

        let billed_durations: Vec<f64> = results
            .warm_starts
            .iter()
            .map(|m| m.billed_duration as f64)
            .collect();
        print_stats(&calculate_stats(&billed_durations), "Billed Duration");

        let memory_used: Vec<f64> = results
            .warm_starts
            .iter()
            .map(|m| m.max_memory_used as f64)
            .collect();
        print_memory_stats(&calculate_stats(&memory_used), "Memory Used");
    }

    if !results.client_measurements.is_empty() {
        println!(
            "\n‚è±Ô∏è Client Metrics ({} invocations) | Memory Size: {} MB",
            results.client_measurements.len(),
            results.client_measurements[0].memory_size
        );
        println!(
            "\n{:<15} {:>12} {:>12} {:>12} {:>12} {:>12}",
            "Metric", "Min", "Max", "Mean", "P50", "P95"
        );
        println!("{:-<87}", "");

        let client_durations: Vec<f64> = results.client_measurements.iter().map(|m| m.client_duration).collect();
        print_stats(&calculate_stats(&client_durations), "Client Duration");
    }
}

pub struct MetricsStats {
    pub min: f64,
    pub max: f64,
    pub mean: f64,
    pub p50: f64,
    pub p95: f64,
}

pub fn calculate_stats(values: &[f64]) -> MetricsStats {
    let mut sorted = values.to_vec();
    sorted.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let len = sorted.len();
    let p95_idx = (len as f64 * 0.95) as usize;

    MetricsStats {
        min: sorted.first().copied().unwrap_or(0.0),
        max: sorted.last().copied().unwrap_or(0.0),
        mean: sorted.iter().sum::<f64>() / len as f64,
        p50: sorted[len / 2],
        p95: sorted[p95_idx],
    }
}

fn print_stats(stats: &MetricsStats, metric: &str) {
    println!(
        "{:<15} {:>9.3} ms  {:>9.3} ms  {:>9.3} ms  {:>9.3} ms  {:>9.3} ms",
        metric, stats.min, stats.max, stats.mean, stats.p50, stats.p95
    );
}

fn print_memory_stats(stats: &MetricsStats, metric: &str) {
    println!(
        "{:<15} {:>9.1} MB  {:>9.1} MB  {:>9.1} MB  {:>9.1} MB  {:>9.1} MB",
        metric, stats.min, stats.max, stats.mean, stats.p50, stats.p95
    );
}

/// Extract function name from ARN
fn extract_function_name(arn: &str) -> &str {
    arn.split(':').last().unwrap_or(arn)
}

pub async fn run_stack_benchmark(
    lambda_client: &LambdaClient,
    cf_client: &CloudFormationClient,
    config: StackBenchmarkConfig,
) -> Result<()> {
    // Get stack outputs
    let response = cf_client
        .describe_stacks()
        .stack_name(&config.stack_name)
        .send()
        .await?;

    let stacks = response
        .stacks
        .ok_or_else(|| anyhow::anyhow!("Stack '{}' not found", config.stack_name))?;

    let outputs = stacks
        .first()
        .ok_or_else(|| anyhow::anyhow!("Stack '{}' not found", config.stack_name))?
        .outputs
        .as_ref()
        .ok_or_else(|| anyhow::anyhow!("Stack '{}' has no outputs", config.stack_name))?;

    // Filter outputs for Lambda function ARNs and apply pattern matching if select is provided
    let functions: Vec<_> = outputs
        .iter()
        .filter(|output| {
            output
                .output_value()
                .map(|v| {
                    v.contains("arn:aws:lambda:")
                        && config.pattern.as_ref().map_or(true, |pattern| v.contains(pattern))
                })
                .unwrap_or(false)
        })
        .map(|output| output.output_value().unwrap())
        .collect();

    if functions.is_empty() {
        return Err(anyhow::anyhow!(
            "No Lambda functions found matching the criteria"
        ));
    }

    println!(
        "\nFound {} functions in stack '{}'",
        functions.len(),
        config.stack_name
    );

    if config.parallel {
        stream::iter(functions)
            .map(|function_arn| {
                let client = lambda_client.clone();
                let env = config.environment.clone();
                let function_name = extract_function_name(function_arn).to_string();
                let memory_size = config.memory_size;
                let concurrent = config.concurrent_invocations as u32;
                let rounds = config.rounds as u32;
                let payload = config.payload.as_deref();
                let output_dir = config.output_dir.clone();
                let client_metrics = config.client_metrics;
                let proxy_function = config.proxy_function.as_deref();
                async move {
                    if is_interrupted() {
                        println!("\nInterrupted, skipping function: {}", function_name);
                        return Err(anyhow!("Benchmark interrupted by user"));
                    }
                    run_function_benchmark(
                        &client,
                        &function_name,
                        memory_size,
                        concurrent,
                        rounds,
                        payload,
                        &output_dir,
                        &env.iter().map(|e| (e.key.as_str(), e.value.as_str())).collect::<Vec<_>>(),
                        client_metrics,
                        proxy_function,
                    )
                    .await
                }
            })
            .buffer_unordered(4)
            .try_for_each(|_| async { Ok(()) })
            .await?;
    } else {
        for function_arn in functions {
            if is_interrupted() {
                println!("\nInterrupted, skipping remaining functions");
                return Err(anyhow!("Benchmark interrupted by user"));
            }
            let function_name = extract_function_name(function_arn);
            run_function_benchmark(
                lambda_client,
                function_name,
                config.memory_size,
                config.concurrent_invocations as u32,
                config.rounds as u32,
                config.payload.as_deref(),
                &config.output_dir,
                &config.environment.iter().map(|e| (e.key.as_str(), e.value.as_str())).collect::<Vec<_>>(),
                config.client_metrics,
                config.proxy_function.as_deref(),
            )
            .await?;
        }
    }

    Ok(())
}
